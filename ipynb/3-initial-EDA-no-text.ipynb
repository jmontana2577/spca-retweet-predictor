{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook 3 - EDA on Non-Text Data and Next Steps\n",
    "The purpose of this notebook is to perform EDA on the non-text aspects of the tweets and create a list of next steps based on findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../data/all_tweets.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**  \n",
    "Checking to see if only the SPCA's are in the username field. Looks like there are others in there. I'm going to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.username.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# created this list by printing unique usernames and erasing non-SPCA's\n",
    "all_sfspcas = ['sfspca', 'PSPCA', 'HoustonSPCA', 'spcaoftexas', 'Tulsa_SPCA', 'RichmondSPCA',\n",
    "              'OntarioSPCA', 'FMSPCA', 'BC_SPCA']\n",
    "\n",
    "df = df.loc[df.username.isin(all_sfspcas)]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**  \n",
    "Because of the boost in retweet numbers created by Hurricane Harvey, I researched other natural disasters in the locations of the organizations and I found a few. I'm going to remove all tweets from a few days before and two weeks following any natural disaster. Here's a list of each disaster:\n",
    "1. Hurricane Harvey (Houston) - made landfall on 8/25/17\n",
    "2. Alberta Wildfire (Fort McMurray) - started 5/1/16 and under control 6/3/16\n",
    "3. Moore Tornado (Oklahoma) - touched down 5/20/13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create disaster dictionary that lists beginning of timeframe as 3 days prior\n",
    "disaster_dict = {\n",
    "    'hurricane': datetime(2017,8,22, tzinfo=pytz.timezone('US/Central')),\n",
    "    'wildfire': datetime(2016,4,29, tzinfo=pytz.timezone('US/Mountain')),\n",
    "    'tornado': datetime(2013,5,18, tzinfo=pytz.timezone('US/Central'))\n",
    "}\n",
    "\n",
    "for disaster, start in disaster_dict.items():\n",
    "    end = start + timedelta(days=14)\n",
    "    df = df.loc[~((df.local_datetime > start) & (df.local_datetime < end))]\n",
    "    print('Shape after removing {}: {}'.format(disaster,df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**  \n",
    "Looking at tweets per user per year in consideration of dropping the first couple years of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "by_year_user = df.groupby(by=['year','username'],as_index=False,sort=False)[['author_id']].count()\n",
    "by_year_user.rename(columns = {'author_id': 'num_tweets'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,6))\n",
    "sns.barplot(x='year', y='num_tweets', hue='username', data=by_year_user, ci=None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**  \n",
    "Dropping all tweets before 2012 since that's when Twitter had IPO and started gaining in popularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[df.year > 2011]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**  \n",
    "Because retweets of a tweet create more potential viewers of the tweet than favoriting a tweet does, I'm going to use number of retweets as the scoring metric (and the target for the model). I will not be using favorites as a predictor because it is the result of a tweet and isn't a known value before the tweet is sent. I will now look at minimizing the effects of outliers by first addressing the skew in the target and then identifying outliers using the Tukey method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# looking at stats for the target\n",
    "df[['retweets']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**  \n",
    "With a lot of the data gathered around 0 and very high maximum relative to the mean, median and standard deviation, this data appears heavily skewed right. Need to transform the data to minimize the skew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get measure of skew to confirm suspicion stated above\n",
    "from scipy.stats import skew\n",
    "\n",
    "retweet_skew = df[['retweets']].apply(lambda x: skew(x))\n",
    "retweet_skew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**  \n",
    "This is a very high skew measurement (as suspected from the max compared to the mean and median), so I'm going to  transform with `log1p` (add 1 and take the natural log)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['retweets'] = np.log1p(df['retweets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get measure of skew again for sake of comparison\n",
    "from scipy.stats import skew\n",
    "\n",
    "retweet_skew = df[['retweets']].apply(lambda x: skew(x))\n",
    "retweet_skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot the distribution of retweets\n",
    "fig = plt.figure(figsize=(8,4))\n",
    "sns.distplot(df.retweets, kde=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**  \n",
    "The transformation has significantly reduced the skew. Time to identify outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to implement the Tukey method for identifying outliers\n",
    "def identify_outliers(dataframe, col):\n",
    "    Q1 = np.percentile(dataframe[col], 25)\n",
    "    Q3 = np.percentile(dataframe[col], 75)\n",
    "    tukey_window = 1.5*(Q3-Q1)\n",
    "    less_than_Q1 = dataframe[col] < Q1 - tukey_window\n",
    "    greater_than_Q3 = dataframe[col] > Q3 + tukey_window\n",
    "    tukey_mask = (less_than_Q1 | greater_than_Q3)\n",
    "    return dataframe[tukey_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outliers = identify_outliers(df,'retweets')\n",
    "outliers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**  \n",
    "I'm going to keep these 12 outliers as the tweets do not look to be the result of anything extraordinary. They include celebrity visitors, campaigns for pet safety, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**  \n",
    "I'm going to create three more columns that are boolean values, one each for whether or not a tweet has a hashtag, a mention or a url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# new columns based on presence of mention, hashtag, url\n",
    "has_one = lambda x: 0 if x == '' else 1\n",
    "df['has_mention'] = df.mentions.apply(has_one)\n",
    "df['has_hashtag'] = df.hashtags.apply(has_one)\n",
    "df['has_url'] = df.urls.apply(has_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20,6))\n",
    "plt.suptitle('Mean and Median Retweets for Each Organization Based on Presence of Mentions', fontsize=20)\n",
    "sns.barplot(x='username', y='retweets', hue='has_mention', data=df, ci=None, ax=axes[0])\n",
    "sns.barplot(x='username', y='retweets', hue='has_mention', data=df, estimator=np.median,ci=None, ax=axes[1])\n",
    "for ax in axes:\n",
    "    plt.sca(ax)\n",
    "    plt.xticks(rotation=45)\n",
    "    ax.yaxis.label.set_fontsize(16)\n",
    "    ax.xaxis.label.set_fontsize(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20,6))\n",
    "plt.suptitle('Mean and Median Retweets for Each Organization Based on Presence of Hashtags', fontsize=20)\n",
    "sns.barplot(x='username', y='retweets', hue='has_hashtag', data=df, ci=None, ax=axes[0])\n",
    "sns.barplot(x='username', y='retweets', hue='has_hashtag', data=df, estimator=np.median,ci=None, ax=axes[1])\n",
    "for ax in axes:\n",
    "    plt.sca(ax)\n",
    "    plt.xticks(rotation=45)\n",
    "    ax.yaxis.label.set_fontsize(16)\n",
    "    ax.xaxis.label.set_fontsize(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,7))\n",
    "plt.suptitle('Retweets by User Based on Presence of Hashtag', fontsize=20)\n",
    "sns.boxplot(x='username', y='retweets', hue='has_hashtag',data=df)\n",
    "plt.xlabel('User', fontsize=16)\n",
    "plt.ylabel('Retweets', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20,6))\n",
    "plt.suptitle('Mean and Median Retweets for Each Organization Based on Presence of URLs', fontsize=20)\n",
    "sns.barplot(x='username', y='retweets', hue='has_url', data=df, ci=None, ax=axes[0])\n",
    "sns.barplot(x='username', y='retweets', hue='has_url', data=df, estimator=np.median,ci=None, ax=axes[1])\n",
    "for ax in axes:\n",
    "    plt.sca(ax)\n",
    "    plt.xticks(rotation=45)\n",
    "    ax.yaxis.label.set_fontsize(16)\n",
    "    ax.xaxis.label.set_fontsize(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20,6))\n",
    "plt.suptitle('Mean and Median Retweets by Hour', fontsize=20)\n",
    "sns.barplot(x='hour', y='retweets', data=df, ci=None, ax=axes[0])\n",
    "sns.barplot(x='hour', y='retweets', data=df, estimator=np.median,ci=None, ax=axes[1])\n",
    "for ax in axes:\n",
    "    plt.sca(ax)\n",
    "    ax.yaxis.label.set_fontsize(16)\n",
    "    ax.xaxis.label.set_fontsize(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,7))\n",
    "plt.suptitle('Retweets by Hour Based on Presence of Hashtag', fontsize=20)\n",
    "sns.boxplot(x='hour', y='retweets', hue='has_hashtag',data=df)\n",
    "plt.xlabel('Hour', fontsize=16)\n",
    "plt.ylabel('Retweets', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at tweets at 3 am to see what's causing such a high value\n",
    "df.loc[df.hour == 3].username.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**  \n",
    "Almost all of the 3 am tweets are from the Alberta organization. Maybe they are using a scheduling tool like zoho or hootsuite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20,6))\n",
    "plt.suptitle('Mean and Median Retweets by Month', fontsize=20)\n",
    "sns.barplot(x='month', y='retweets', data=df, ci=None, ax=axes[0])\n",
    "sns.barplot(x='month', y='retweets', data=df, estimator=np.median,ci=None, ax=axes[1])\n",
    "for ax in axes:\n",
    "    plt.sca(ax)\n",
    "    ax.yaxis.label.set_fontsize(16)\n",
    "    ax.xaxis.label.set_fontsize(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20,6))\n",
    "plt.suptitle('Mean and Median Retweets by Day of Week', fontsize=20)\n",
    "sns.barplot(x='weekday', y='retweets', data=df, ci=None, ax=axes[0])\n",
    "sns.barplot(x='weekday', y='retweets', data=df, estimator=np.median,ci=None, ax=axes[1])\n",
    "for ax in axes:\n",
    "    plt.sca(ax)\n",
    "    ax.yaxis.label.set_fontsize(16)\n",
    "    ax.xaxis.label.set_fontsize(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:**  \n",
    "1. Having a mention seems to result in lower retweets.\n",
    "1. Having a hashtag seems to result in higher retweets, even more so for some organizations (i.e. BC SPCA) and the increase varies a lot by time of day too.\n",
    "1. Having a url has varying affects depending on organization.\n",
    "1. It appears tweets sent in the very early morning hours have the highest number of retweets.\n",
    "1. Tweets sent in July got a little bump in retweets, and it's interesting to note that none of the outliers were tweeted in July.\n",
    "1. It looks like weekend tweets get more retweets than those sent during the week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Next Steps\n",
    "1. Can I get number of followers from the Twitter API?  \n",
    "**Result:** Yes, but only back to mid-2016  \n",
    "\n",
    "1. Look into timezone of timestamp so that \"hour\" is local to user and day of week is local too. Might need to add this as tweets are pulled since adjustment for each SPCA may be different. Also got lots of tweets at 3 am from Alberta organization. Can this be real?  \n",
    "**Result:** Fixed time zone issue and graph remained relatively unchanged. I suspect they're using a scheduling tool to send tweets in the wee hours.  \n",
    "\n",
    "1. Investigate how those 11 tweets from non-SPCA users got into the data.  \n",
    "**Result:** Not worth investigating. Just gonna drop them.  \n",
    "\n",
    "1. Drop the tweets from Houston since Hurricane Harvey.  \n",
    "**Result:** Dropped all the tweets during the time periods of three major natural disasters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_pickle('../data/post_eda.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
