{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook 4 - Text Processing\n",
    "The purpose of this notebook is to process the text portion of the tweets in preparation for training models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../data/3-post_eda.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**  \n",
    "Taking a look at some of the text data to see what sort of cleaning needs to be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('\\n\\n'.join(df.text.sample(10).values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of considerations for text cleaning\n",
    "1. **Hashtags and At Symbols:** removing just the symbols themselves, but keeping the phrases attached. Hashtags and mentions are also going to be in a separate data set on which to fit a model and ensemble with other data sets.\n",
    "1. **URL's:** I will get rid of them entirely. Need to note that there are some that begin with \"http\" and some that do not. Email addresses should be get the same treatment.\n",
    "1. **Punctuation:** I will remove all punctuation, which will capture the hashtags and at symbols mentioned above.\n",
    "1. **Capital Letters:** I will convert everything to lower case.\n",
    "1. **Numbers:** I'm going to replace stand-alone numbers to the string \"NUMBER\", but numbers part of a string will remain. For instance, 280 will become NUMBER but kourtneeybell3 will stay the same.\n",
    "1. **Whitespace:** All white space will be replaced with a single space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to clean the input text\n",
    "def cleaner(message):\n",
    "    message = re.sub('https?:\\/{2}\\s?[^\\s]*', '', message) # remove http url's\n",
    "    message = re.sub('[^\\s]+\\/[^\\s]+', '', message) # remove some random strings with /'s\n",
    "    message = re.sub('[^\\s]*\\.com[^\\s]*', '', message) # remove .com that doesn't start with http\n",
    "    message = re.sub('[^\\s]*\\.net[^\\s]*', '', message) # remove .net that doesn't start with http\n",
    "    message = re.sub('\\.+', ' ', message) # replace dots with space\n",
    "    message = re.sub('[^a-z0-9 ]','', message.lower())  # convert to lowercase and remove punctuation\n",
    "    message = re.sub('\\s+\\d+\\s+',' NUMBER ',message) # replace stand-alone numbers with the string \"NUMBER\"\n",
    "    message = re.sub('\\s+',' ',message) # replace whitespace with a single space\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "some_tweet_text = df.text.sample(10).values\n",
    "for t in some_tweet_text:\n",
    "    print(t)\n",
    "    print(cleaner(t), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Note:**  \n",
    "Based on several 10-tweet samples, the cleaner appears to be working quite well. One issue I notice is with url's that have multiple spaces. I don't believe there's a way to eliminate those without also eliminating relevant text that follows a url. Here's an example:  \n",
    "\n",
    "```\n",
    "RT @aerocar: @aerocar & @HighendLimo are proud supporters of @pawsforacause ! http:// support.spca.bc.ca/site/TR?pg=ent ry&fr_id=1424 â€¦\n",
    "\n",
    "rt aerocar aerocar highendlimo are proud supporters of pawsforacause ryfrid1424\n",
    "```\n",
    "\n",
    "Overall, I believe the cleaner works sufficiently so I'm going to clean it and write it to a pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['text'] = df.text.apply(cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.loc[:,['text', 'retweets']]\n",
    "df.to_pickle('../data/4-clean_text.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a Linear Regression Model \n",
    "Split the data into training and test sets, being sure to set a random seed for reproducibility. Then build a pipeline to gridsearch the following:  \n",
    "1. Feature extraction using `TfidfVectorizer`.\n",
    "1. Dimensionality reduction using `TruncatedSVD`.\n",
    "1. Regression using `LinearRegression`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.text, df.retweets, test_size=.3, random_state=55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the pipeline\n",
    "linreg_pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words='english')),\n",
    "    ('svd', TruncatedSVD()),\n",
    "    ('regr', LinearRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the parameters dictionary\n",
    "linreg_params = {\n",
    "    'tfidf__norm': ['l1', 'l2'],\n",
    "    'tfidf__ngram_range': [(1,1), (1,2), (1,3)],\n",
    "    'tfidf__min_df': [1, 3, 5],\n",
    "    'svd__n_components': [2, 10, 20]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# perform the grid search\n",
    "linreg_gs = GridSearchCV(linreg_pipe, linreg_params, n_jobs=-1, verbose=1)\n",
    "linreg_gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "def get_gs_results(params, model, xtest, ytest):\n",
    "    print('Parameters used:')\n",
    "    pprint(linreg_params)\n",
    "    best_parameters = model.best_estimator_.get_params()\n",
    "    print('Best parameters:')\n",
    "    for p_name in sorted(params.keys()):\n",
    "        print(\"\\t{}: {}\".format(p_name, best_parameters[p_name]))\n",
    "    print('Train score: {}'.format(model.best_score_))\n",
    "    print('Test score: {}'.format(model.score(xtest,ytest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_gs_results(linreg_params, linreg_gs, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(linreg_gs.cv_results_).sort_values('rank_test_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
