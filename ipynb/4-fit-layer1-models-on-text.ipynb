{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook 4 - Fit Regression Models on the Tweet Text\n",
    "The purpose of this notebook is to process the text portion of the tweets (feature extraction and dimensionality reduction) and train regression models for the first layer of the model stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.ensemble import AdaBoostRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Text Encoder\n",
    "I gathered over one million random tweets from around the US and Canada to use for fitting the text encoder. I did this using the Twitter Stream API and two different AWS instances. To avoid duplicates, one instance pulled tweets from the left half of US and Canada (labeled 'west') and the other pulled from the right half (labeled 'east'). Consult the notebook titled `1-get-tweets-streaming` in the `ipynb` folder for the code. The random tweets are all stored in pickle files (roughly 10,000 tweets per file with some exceptions). Here are the necessary steps to train the encoder:  \n",
    "1. Import text from tweets and create a huge dataframe.\n",
    "1. Clean the text using the cleaner.\n",
    "1. Fit the tf-idf vectorizer using the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # one-time code to import the files that start with east (68 in all)\n",
    "# df = pd.DataFrame()\n",
    "# path = '../data/'\n",
    "# for i in range(69):\n",
    "#     print(i, end='\\r')\n",
    "#     if i < 10:\n",
    "#         filename = path + 'east-0{}.p'.format(i)\n",
    "#     else:\n",
    "#         filname = path + 'east-{}.p'.format(i)\n",
    "    \n",
    "#     df = df.append(pd.read_pickle(filename), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # one-time code to import the files that start with west (35 in all) and append to the previous dataframe\n",
    "# path = '../data/'\n",
    "# for i in range(36):\n",
    "#     print(i, end='\\r')\n",
    "#     if i < 10:\n",
    "#         filename = path + 'west-0{}.p'.format(i)\n",
    "#     else:\n",
    "#         filname = path + 'west-{}.p'.format(i)\n",
    "    \n",
    "#     df = df.append(pd.read_pickle(filename), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # drop rows with empty tweets\n",
    "# df.dropna(subset=['text'], inplace=True)\n",
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # write to pickle so that all the individual files can be deleted\n",
    "# df.to_pickle('../data/big_tweets.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1034999, 1), (80836, 19))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_big = pd.read_pickle('../data/big_tweets.p')\n",
    "df_spca = pd.read_pickle('../data/3-post_eda.p')\n",
    "df_big.shape, df_spca.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**  \n",
    "Taking a look at some of the text data to see what sort of cleaning needs to be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('\\n\\n'.join(df_spca.text.sample(10).values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of considerations for text cleaning\n",
    "1. **Hashtags and At Symbols:** removing just the symbols themselves, but keeping the phrases attached. Hashtags and mentions are also going to be in a separate data set on which to fit a model and ensemble with other data sets.\n",
    "1. **URL's:** I will get rid of them entirely. Need to note that there are some that begin with \"http\" and some that do not. Email addresses should be get the same treatment.\n",
    "1. **Punctuation:** I will remove all punctuation, which will capture the hashtags and at symbols mentioned above.\n",
    "1. **Capital Letters:** I will convert everything to lower case.\n",
    "1. **Numbers:** I'm going to replace stand-alone numbers to the string \"NUMBER\", but numbers part of a string will remain. For instance, 280 will become NUMBER but kourtneeybell3 will stay the same.\n",
    "1. **Whitespace:** All white space will be replaced with a single space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to clean the input text\n",
    "def cleaner(message):\n",
    "    message = re.sub('https?:\\/{2}\\s?[^\\s]*', '', message) # remove http url's\n",
    "    message = re.sub('[^\\s]+\\/[^\\s]+', '', message) # remove some random strings with /'s\n",
    "    message = re.sub('[^\\s]*\\.com[^\\s]*', '', message) # remove .com that doesn't start with http\n",
    "    message = re.sub('[^\\s]*\\.net[^\\s]*', '', message) # remove .net that doesn't start with http\n",
    "    message = re.sub('\\.+', ' ', message) # replace dots with space\n",
    "    message = re.sub('[^a-z0-9 ]','', message.lower())  # convert to lowercase and remove punctuation\n",
    "    message = re.sub('\\s+\\d+\\s+',' NUMBER ',message) # replace stand-alone numbers with the string \"NUMBER\"\n",
    "    message = re.sub('\\s+',' ',message) # replace whitespace with a single space\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compare pre-processed text to clean text to validate the cleaner function\n",
    "some_tweet_text = df_spca.text.sample(10).values\n",
    "for t in some_tweet_text:\n",
    "    print(t)\n",
    "    print(cleaner(t), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Note:**  \n",
    "Based on several 10-tweet samples, the cleaner appears to be working quite well. One issue I notice is with url's that have multiple spaces. I don't believe there's a way to eliminate those without also eliminating relevant text that follows a url. Here's an example:  \n",
    "\n",
    "```\n",
    "RT @aerocar: @aerocar & @HighendLimo are proud supporters of @pawsforacause ! http:// support.spca.bc.ca/site/TR?pg=ent ry&fr_id=1424 â€¦\n",
    "\n",
    "rt aerocar aerocar highendlimo are proud supporters of pawsforacause ryfrid1424\n",
    "```\n",
    "\n",
    "Overall, I believe the cleaner works sufficiently so time to clean the text for both dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_big['text'] = df_big.text.apply(cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_spca['text'] = df_spca.text.apply(cleaner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Split\n",
    "Because I intend to use stacking, I have to split the approximate 80k rows of data as follows:\n",
    "- 50k to be used to train (40k) and test (10k) the models in the first layer of the stack.  \n",
    "- 30k to be used to train (24k) and test (6k) the blender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40740, 18) (10186, 18) (40740,) (10186,)\n",
      "(23928, 18) (5982, 18) (23928,) (5982,)\n"
     ]
    }
   ],
   "source": [
    "# split the data into layer1 set (63%) and blender set (37%) \n",
    "df_layer1, df_blender, y_layer1, y_blender = train_test_split(df_spca.drop('retweets', axis=1), df_spca.retweets, \n",
    "                                                              test_size=.37, random_state=55)\n",
    "\n",
    "# split each of the layer1 set and blender set into train and test sets\n",
    "df_layer1_train, df_layer1_test, y_layer1_train, y_layer1_test = train_test_split(df_layer1, y_layer1, \n",
    "                                                                                  test_size=.2, random_state=55)\n",
    "df_blender_train, df_blender_test, y_blender_train, y_blender_test = train_test_split(df_blender, y_blender, \n",
    "                                                                                      test_size=.2, random_state=55)\n",
    "\n",
    "print(df_layer1_train.shape, df_layer1_test.shape, y_layer1_train.shape, y_layer1_test.shape)\n",
    "print(df_blender_train.shape, df_blender_test.shape, y_blender_train.shape, y_blender_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# output the split data to pickle files for use in other notebooks\n",
    "joblib.dump(df_layer1_train, '../data/df_layer1_train.pkl')\n",
    "joblib.dump(df_layer1_test, '../data/df_layer1_test.pkl')\n",
    "joblib.dump(y_layer1_train, '../data/y_layer1_train.pkl')\n",
    "joblib.dump(y_layer1_test, '../data/y_layer1_test.pkl')\n",
    "joblib.dump(df_blender_train, '../data/df_blender_train.pkl')\n",
    "joblib.dump(df_blender_test, '../data/df_blender_test.pkl')\n",
    "joblib.dump(y_blender_train, '../data/y_blender_train.pkl')\n",
    "joblib.dump(y_blender_test, '../data/y_blender_test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Various Regressors on the Text Data to Identify Best Options for Layer 1\n",
    "1. Fit the `TfidfVectorizer` to the random tweets with `stop_words='english'` and everything else is default for the first run. Repeat with different values for `ngram_range`, `min_df` and `max_features`.\n",
    "1. Grab just the SPCA text data as the predictor.  \n",
    "1. Transform the SPCA training text using the fit tfidf vectorizer and svd.\n",
    "1. Train various regressors to see which are most appropriate to use for the first layer. I will try the following regressors with default parameters, and the intention is to ultimately boost those that lead to best results:  \n",
    "    1. `Lasso`\n",
    "    1. `DecisionTreeRegressor`\n",
    "    1. `KNeighborsRegressor`\n",
    "    1. `BayesianRidge`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import Lasso, BayesianRidge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1034999, 136354)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=500, n_iter=5,\n",
       "       random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit random twitter text data with tfidf and svd\n",
    "tfidf = TfidfVectorizer(stop_words='english', ngram_range=(1,1), min_df=1, max_features=None)\n",
    "big_sparse = tfidf.fit_transform(df_big.text)\n",
    "display(big_sparse.shape)\n",
    "svd = TruncatedSVD(500)\n",
    "svd.fit(big_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the text data from the train and test sets for first layer\n",
    "X_layer1_train = df_layer1_train.text\n",
    "X_layer1_test = df_layer1_test.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to fit and score a regressor\n",
    "def fit_and_score(data, regr, return_regr=False):\n",
    "    '''\n",
    "    Transforms the data using the pre-fit tfidf and svd, fits the regressor to the transformed \n",
    "    training data and then prints train and test scores.\n",
    "    \n",
    "    Parameters:\n",
    "        data - iterable containing X_train, X_test, y_train, y_test\n",
    "        regr - instantiated regressor\n",
    "        return_regr - boolean, option to return the fit regressor (default: False)\n",
    "    \n",
    "    Returns: optional, regressor fit to the transformed training data\n",
    "    '''\n",
    "    \n",
    "    print('Regressor: {}'.format(regr))\n",
    "    train_sparse = tfidf.transform(data[0])\n",
    "    X_train = svd.transform(train_sparse)\n",
    "    \n",
    "    regr.fit(X_train, data[2])\n",
    "    print('Train score: {}'.format(regr.score(X_train, data[2])))\n",
    "    \n",
    "    test_sparse = tfidf.transform(data[1])\n",
    "    X_test = svd.transform(test_sparse)\n",
    "    \n",
    "    print('Test score: {}'.format(regr.score(X_test, data[3])))\n",
    "    \n",
    "    if return_regr:\n",
    "        return regr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (X_layer1_train, X_layer1_test, y_layer1_train, y_layer1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lasso = Lasso()\n",
    "dtr = DecisionTreeRegressor()\n",
    "knr = KNeighborsRegressor(n_jobs=-1)\n",
    "bayes = BayesianRidge()\n",
    "models = [lasso, dtr, knr, bayes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regressor: Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=None,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Train score: 0.0\n",
      "Test score: -2.8063777695486184e-05\n",
      "\n",
      "Regressor: DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "           max_leaf_nodes=None, min_impurity_split=1e-07,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "           splitter='best')\n",
      "Train score: 0.8982282233729353\n",
      "Test score: 0.0029356071510547865\n",
      "\n",
      "Regressor: KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
      "          weights='uniform')\n",
      "Train score: 0.5520648124327616\n",
      "Test score: 0.3402886119190035\n",
      "\n",
      "Regressor: BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True,\n",
      "       fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06, n_iter=300,\n",
      "       normalize=False, tol=0.001, verbose=False)\n",
      "Train score: 0.35062652023944924\n",
      "Test score: 0.3315576797686248\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    fit_and_score(data, model)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tfidf with max_features = 50000\n",
    "tfidf = TfidfVectorizer(stop_words='english', ngram_range=(1,1), min_df=1, max_features=50000)\n",
    "big_sparse = tfidf.fit_transform(df_big.text)\n",
    "display(big_sparse.shape)\n",
    "svd = TruncatedSVD(n_components=500)\n",
    "svd.fit(big_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    fit_and_score(data, model)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tfidf with ngram_range = (2,2)\n",
    "tfidf = TfidfVectorizer(stop_words='english', ngram_range=(2,2), min_df=1, max_features=None)\n",
    "big_sparse = tfidf.fit_transform(df_big.text)\n",
    "display(big_sparse.shape)\n",
    "svd = TruncatedSVD(n_components=500)\n",
    "svd.fit(big_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    fit_and_score(data, model)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1034999, 137053)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=300, n_iter=5,\n",
       "       random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# svd with n_components=300\n",
    "tfidf = TfidfVectorizer(stop_words='english', ngram_range=(1,1), min_df=1, max_features=None)\n",
    "big_sparse = tfidf.fit_transform(df_big.text)\n",
    "display(big_sparse.shape)\n",
    "svd = TruncatedSVD(300)\n",
    "svd.fit(big_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regressor: Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=None,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Train score: 0.0\n",
      "Test score: -2.8063777695486184e-05\n",
      "\n",
      "Regressor: DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "           max_leaf_nodes=None, min_impurity_split=1e-07,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "           splitter='best')\n",
      "Train score: 0.8982212699944269\n",
      "Test score: -0.022639248855147454\n",
      "\n",
      "Regressor: KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
      "          weights='uniform')\n",
      "Train score: 0.5435563152137841\n",
      "Test score: 0.3290599948063848\n",
      "\n",
      "Regressor: BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True,\n",
      "       fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06, n_iter=300,\n",
      "       normalize=False, tol=0.001, verbose=False)\n",
      "Train score: 0.3215033668456203\n",
      "Test score: 0.3072544969456539\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    fit_and_score(data, model)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tfidf and SVD Tuning Attempts**  \n",
    "1. `TfidfVectorizer(min_df = 10)` - ever so slightly worse\n",
    "1. `TfidfVectorizer(max_features = 50000)` - ever so slightly worse\n",
    "1. `TfidfVectorizer(ngram_range = (2,2))` - significantly worse\n",
    "1. `TruncatedSVD(n_components=300)` - slightly worse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressor Boosting\n",
    "Based on the above findings, the two regressors that I will try to boost are KNeighbors and BayesianRidge. I will use the stock `TfidfVectorizer` and `TruncatedSVD(n_components=500)` that were fit on the random tweet text to encode the SPCA tweet text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit random text data with tfidf and svd\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "big_sparse = tfidf.fit_transform(df_big.text)\n",
    "display(big_sparse.shape)\n",
    "svd = TruncatedSVD(500)\n",
    "svd.fit(big_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/fit_svd.pkl']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pickle the tfidf and svd for later use\n",
    "joblib.dump(tfidf, '../data/4-text_tfidf.pkl')\n",
    "joblib.dump(svd, '../data/4-text_svd.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the text data from the train and test sets for first layer\n",
    "X_layer1_train = df_layer1_train.text\n",
    "X_layer1_test = df_layer1_test.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transform the training text and test text\n",
    "train_sparse = tfidf.transform(X_layer1_train)\n",
    "X_layer1_train = svd.transform(train_sparse)\n",
    "\n",
    "test_sparse = tfidf.transform(X_layer1_test)\n",
    "X_layer1_test = svd.transform(test_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ada = AdaBoostRegressor(KNeighborsRegressor(n_jobs=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostRegressor(base_estimator=KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "          metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
       "          weights='uniform'),\n",
       "         learning_rate=1.0, loss='linear', n_estimators=50,\n",
       "         random_state=None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada.fit(X_layer1_train, y_layer1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/4-text_ada_50.pkl']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pickle the ada since it took 4 hours to fit\n",
    "joblib.dump(ada, '../data/4-text_ada_50.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-90a0061d95d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mada\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_layer1_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_layer1_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m         return r2_score(y, self.predict(X), sample_weight=sample_weight,\n\u001b[0m\u001b[1;32m    387\u001b[0m                         multioutput='variance_weighted')\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1089\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1091\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_median_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstaged_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py\u001b[0m in \u001b[0;36m_get_median_predict\u001b[0;34m(self, X, limit)\u001b[0m\n\u001b[1;32m   1054\u001b[0m         \u001b[0;31m# Evaluate predictions of all estimators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m         predictions = np.array([\n\u001b[0;32m-> 1056\u001b[0;31m             est.predict(X) for est in self.estimators_[:limit]]).T\n\u001b[0m\u001b[1;32m   1057\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m         \u001b[0;31m# Sort the predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1054\u001b[0m         \u001b[0;31m# Evaluate predictions of all estimators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m         predictions = np.array([\n\u001b[0;32m-> 1056\u001b[0;31m             est.predict(X) for est in self.estimators_[:limit]]).T\n\u001b[0m\u001b[1;32m   1057\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m         \u001b[0;31m# Sort the predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/neighbors/regression.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mneigh_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneigh_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneigh_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/neighbors/base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    379\u001b[0m                 delayed(self._tree.query, check_pickle=False)(\n\u001b[1;32m    380\u001b[0m                     X[s], n_neighbors, return_distance)\n\u001b[0;32m--> 381\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen_even_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m             )\n\u001b[1;32m    383\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreturn_distance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    766\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    717\u001b[0m                     \u001b[0mensure_ready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m                     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabort_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    680\u001b[0m                 \u001b[0;31m# check if timeout supported in backend future implementation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m'timeout'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetfullargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ada.score(X_layer1_train, y_layer1_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code from when I thought I was tuning models using GridSearchCV. Eventually delete this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the parameters for each of the regressors\n",
    "knr_params = {\n",
    "    'n_neighbors': np.arange(1, 17, 2),\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'p': [1, 2, 3] # p for minkowski where 1 is manhattan, 2 is euclidian\n",
    "}\n",
    "\n",
    "rfr_params = {\n",
    "    'n_estimators': np.arange(5,30,5)\n",
    "}\n",
    "\n",
    "br_params = {\n",
    "    'alpha_1': np.logspace(-6, -5, 2),\n",
    "    'alpha_2': np.logspace(-6, -5, 2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rs_knr = RandomizedSearchCV(knr, knr_params, n_jobs=-1, verbose=1)\n",
    "rs_knr.fit(X_train, y_train)\n",
    "display(rs_knr.score(X_train, y_train))\n",
    "display(rs_knr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfr = RandomForestRegressor(n_jobs=-1)\n",
    "rs_rfr = RandomizedSearchCV(rfr, rfr_params, n_jobs=-1, verbose=1)\n",
    "rs_rfr.fit(X_train, y_train)\n",
    "display(rs_rfr.score(X_train, y_train))\n",
    "display(rs_rfr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rs_br = RandomizedSearchCV(br, br_params, n_jobs=-1, verbose=1)\n",
    "rs_br.fit(X_train, y_train)\n",
    "display(rs_br.score(X_train, y_train))\n",
    "display(rs_br.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs_knr = GridSearchCV(knr, knr_params, n_jobs=-1, verbose=1)\n",
    "gs_knr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
