{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook 4 - Fit Regression Models on the Tweet Text\n",
    "The purpose of this notebook is to process the text portion of the tweets (feature extraction and dimensionality reduction) and train regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Text Encoder\n",
    "I gathered over one million random tweets from around the US and Canada to use for fitting the text encoder. I did this using the Twitter Stream API and two different AWS instances. To avoid duplicates, one instance pulled tweets from the left half of US and Canada (labeled 'west') and the other pulled from the right half (labeled 'east'). Consult the notebook titled `1-get-tweets-streaming` in the `ipynb` folder for the code. The random tweets are all stored in pickle files (roughly 10,000 tweets per file with some exceptions). Here are the necessary steps to train the encoder:  \n",
    "1. Import text from tweets and create a huge dataframe.\n",
    "1. Clean the text using the cleaner.\n",
    "1. Fit the tf-idf vectorizer using the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # one-time code to import the files that start with east (68 in all)\n",
    "# df = pd.DataFrame()\n",
    "# path = '../data/'\n",
    "# for i in range(69):\n",
    "#     print(i, end='\\r')\n",
    "#     if i < 10:\n",
    "#         filename = path + 'east-0{}.p'.format(i)\n",
    "#     else:\n",
    "#         filname = path + 'east-{}.p'.format(i)\n",
    "    \n",
    "#     df = df.append(pd.read_pickle(filename), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # one-time code to import the files that start with west (35 in all) and append to the previous dataframe\n",
    "# path = '../data/'\n",
    "# for i in range(36):\n",
    "#     print(i, end='\\r')\n",
    "#     if i < 10:\n",
    "#         filename = path + 'west-0{}.p'.format(i)\n",
    "#     else:\n",
    "#         filname = path + 'west-{}.p'.format(i)\n",
    "    \n",
    "#     df = df.append(pd.read_pickle(filename), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # drop rows with empty tweets\n",
    "# df.dropna(subset=['text'], inplace=True)\n",
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # write to pickle so that all the individual files can be deleted\n",
    "# df.to_pickle('../data/big_tweets.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1034999, 35), (80836, 19))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_big = pd.read_pickle('../data/big_tweets.p')\n",
    "df_spca = pd.read_pickle('../data/3-post_eda.p')\n",
    "df_big.shape, df_spca.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**  \n",
    "Taking a look at some of the text data to see what sort of cleaning needs to be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('\\n\\n'.join(df_spca.text.sample(10).values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of considerations for text cleaning\n",
    "1. **Hashtags and At Symbols:** removing just the symbols themselves, but keeping the phrases attached. Hashtags and mentions are also going to be in a separate data set on which to fit a model and ensemble with other data sets.\n",
    "1. **URL's:** I will get rid of them entirely. Need to note that there are some that begin with \"http\" and some that do not. Email addresses should be get the same treatment.\n",
    "1. **Punctuation:** I will remove all punctuation, which will capture the hashtags and at symbols mentioned above.\n",
    "1. **Capital Letters:** I will convert everything to lower case.\n",
    "1. **Numbers:** I'm going to replace stand-alone numbers to the string \"NUMBER\", but numbers part of a string will remain. For instance, 280 will become NUMBER but kourtneeybell3 will stay the same.\n",
    "1. **Whitespace:** All white space will be replaced with a single space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to clean the input text\n",
    "def cleaner(message):\n",
    "    message = re.sub('https?:\\/{2}\\s?[^\\s]*', '', message) # remove http url's\n",
    "    message = re.sub('[^\\s]+\\/[^\\s]+', '', message) # remove some random strings with /'s\n",
    "    message = re.sub('[^\\s]*\\.com[^\\s]*', '', message) # remove .com that doesn't start with http\n",
    "    message = re.sub('[^\\s]*\\.net[^\\s]*', '', message) # remove .net that doesn't start with http\n",
    "    message = re.sub('\\.+', ' ', message) # replace dots with space\n",
    "    message = re.sub('[^a-z0-9 ]','', message.lower())  # convert to lowercase and remove punctuation\n",
    "    message = re.sub('\\s+\\d+\\s+',' NUMBER ',message) # replace stand-alone numbers with the string \"NUMBER\"\n",
    "    message = re.sub('\\s+',' ',message) # replace whitespace with a single space\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compare pre-processed text to clean text to validate the cleaner function\n",
    "some_tweet_text = df_spca.text.sample(10).values\n",
    "for t in some_tweet_text:\n",
    "    print(t)\n",
    "    print(cleaner(t), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Note:**  \n",
    "Based on several 10-tweet samples, the cleaner appears to be working quite well. One issue I notice is with url's that have multiple spaces. I don't believe there's a way to eliminate those without also eliminating relevant text that follows a url. Here's an example:  \n",
    "\n",
    "```\n",
    "RT @aerocar: @aerocar & @HighendLimo are proud supporters of @pawsforacause ! http:// support.spca.bc.ca/site/TR?pg=ent ry&fr_id=1424 â€¦\n",
    "\n",
    "rt aerocar aerocar highendlimo are proud supporters of pawsforacause ryfrid1424\n",
    "```\n",
    "\n",
    "Overall, I believe the cleaner works sufficiently so time to clean the text for both dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_big['text'] = df_big.text.apply(cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_spca['text'] = df_spca.text.apply(cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split the data into train and test sets\n",
    "df_train, df_test, y_train, y_test = train_test_split(df_spca.drop('retweets', axis=1), df_spca.retweets, \n",
    "                                                      test_size=.3, random_state=55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Various Regressors on the Text Data to Identify Best Options \n",
    "1. Fit the `TfidfVectorizer` to the random tweets with `stop_words='english'` and everything else is default.\n",
    "1. Grab just the SPCA text data as the predictor.  \n",
    "1. Transform the SPCA training text using the fit tfidf vectorizer. Check the dimensions of the sparse matrix to see if adjustments need to be made to the vectorizer (i.e. ngram_range, min_df, max_features)\n",
    "1. Perform dimensionality reduction using `TruncatedSVD` with all components.\n",
    "1. Train various regressors to see which are most appropriate to try to tune. I will try the following regressors:  \n",
    "    1. `Lasso`\n",
    "    1. `DecisionTreeRegressor`\n",
    "    1. `KNeighborsRegressor`\n",
    "    1. `BayesianRidge`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LinearRegression, Lasso, BayesianRidge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1034999, 137053)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=500, n_iter=5,\n",
       "       random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit random text data with tfidf and svd\n",
    "tfidf = TfidfVectorizer(stop_words='english', ngram_range=(1,1), min_df=1, max_features=None)\n",
    "big_sparse = tfidf.fit_transform(df_big.text)\n",
    "display(big_sparse.shape)\n",
    "svd = TruncatedSVD(500)\n",
    "svd.fit(big_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# free up the space occupied by these large objects\n",
    "del df_big\n",
    "del big_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the text data from the train and test sets\n",
    "X_train = df_train.text\n",
    "X_test = df_test.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_and_score(data, regr):\n",
    "    print('Regressor: {}'.format(regr))\n",
    "    train_sparse = tfidf.transform(data[0])\n",
    "    X_train = svd.transform(train_sparse)\n",
    "    \n",
    "    regr.fit(X_train, data[2])\n",
    "    print('Train score: {}'.format(regr.score(X_train, data[2])))\n",
    "    \n",
    "    test_sparse = tfidf.transform(data[1])\n",
    "    X_test = svd.transform(test_sparse)\n",
    "    \n",
    "    print('Test score: {}'.format(regr.score(X_test, data[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = (X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lasso = Lasso()\n",
    "dtr = DecisionTreeRegressor()\n",
    "knr = KNeighborsRegressor(n_jobs=-1)\n",
    "bayes = BayesianRidge()\n",
    "models = [lasso, dtr, knr, bayes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regressor: Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=None,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Train score: 0.0\n",
      "Test score: -2.8063777695486184e-05\n",
      "\n",
      "Regressor: DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "           max_leaf_nodes=None, min_impurity_split=1e-07,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "           splitter='best')\n",
      "Train score: 0.8982282233729353\n",
      "Test score: 0.0029356071510547865\n",
      "\n",
      "Regressor: KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
      "          weights='uniform')\n",
      "Train score: 0.5520648124327616\n",
      "Test score: 0.3402886119190035\n",
      "\n",
      "Regressor: BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True,\n",
      "       fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06, n_iter=300,\n",
      "       normalize=False, tol=0.001, verbose=False)\n",
      "Train score: 0.35062652023944924\n",
      "Test score: 0.3315576797686248\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    fit_and_score(data, model)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tfidf with max_features = 50000\n",
    "tfidf = TfidfVectorizer(stop_words='english', ngram_range=(1,1), min_df=1, max_features=50000)\n",
    "big_sparse = tfidf.fit_transform(df_big.text)\n",
    "display(big_sparse.shape)\n",
    "svd = TruncatedSVD(n_components=500)\n",
    "svd.fit(big_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    fit_and_score(data, model)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tfidf with ngram_range = (2,2)\n",
    "tfidf = TfidfVectorizer(stop_words='english', ngram_range=(2,2), min_df=1, max_features=None)\n",
    "big_sparse = tfidf.fit_transform(df_big.text)\n",
    "display(big_sparse.shape)\n",
    "svd = TruncatedSVD(n_components=500)\n",
    "svd.fit(big_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    fit_and_score(data, model)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1034999, 137053)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=300, n_iter=5,\n",
       "       random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# svd with n_components=300\n",
    "tfidf = TfidfVectorizer(stop_words='english', ngram_range=(1,1), min_df=1, max_features=None)\n",
    "big_sparse = tfidf.fit_transform(df_big.text)\n",
    "display(big_sparse.shape)\n",
    "svd = TruncatedSVD(300)\n",
    "svd.fit(big_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regressor: Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=None,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Train score: 0.0\n",
      "Test score: -2.8063777695486184e-05\n",
      "\n",
      "Regressor: DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "           max_leaf_nodes=None, min_impurity_split=1e-07,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "           splitter='best')\n",
      "Train score: 0.8982212699944269\n",
      "Test score: -0.022639248855147454\n",
      "\n",
      "Regressor: KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
      "          weights='uniform')\n",
      "Train score: 0.5435563152137841\n",
      "Test score: 0.3290599948063848\n",
      "\n",
      "Regressor: BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True,\n",
      "       fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06, n_iter=300,\n",
      "       normalize=False, tol=0.001, verbose=False)\n",
      "Train score: 0.3215033668456203\n",
      "Test score: 0.3072544969456539\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    fit_and_score(data, model)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tuning Attempts**  \n",
    "1. `TfidfVectorizer(min_df = 10)` - ever so slightly worse\n",
    "1. `TfidfVectorizer(max_features = 50000)` - ever so slightly worse\n",
    "1. `TfidfVectorizer(ngram_range = (2,2))` - significantly worse\n",
    "1. `TruncatedSVD(n_components=300)` - slightly worse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regressor: RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=10, n_jobs=-1, oob_score=False, random_state=None,\n",
      "           verbose=0, warm_start=False)\n",
      "Train score: 0.8125377674505\n",
      "Test score: 0.3877395574162248\n"
     ]
    }
   ],
   "source": [
    "fit_and_score(data, RandomForestRegressor(n_jobs=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressor Tuning\n",
    "Based on the above findings, the two regressors that I will try to tune are KNeighbors and BayesianRidge. I will use the stock `TfidfVectorizer` and `TruncatedSVD(n_components=500)` fit on the random tweet text to encode the SPCA tweet text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit random text data with tfidf and svd\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "big_sparse = tfidf.fit_transform(df_big.text)\n",
    "display(big_sparse.shape)\n",
    "svd = TruncatedSVD(500)\n",
    "svd.fit(big_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the text data from the train and test sets\n",
    "X_train = df_train.text\n",
    "X_test = df_test.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transform the training text and test text\n",
    "train_sparse = tfidf.transform(X_train)\n",
    "X_train = svd.transform(train_sparse)\n",
    "\n",
    "test_sparse = tfidf.transform(X_test)\n",
    "X_test = svd.transform(test_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the parameters for each of the regressors\n",
    "knr_params = {\n",
    "    'n_neighbors': np.arange(1, 17, 2),\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'p': [1, 2, 3] # p for minkowski where 1 is manhattan, 2 is euclidian\n",
    "}\n",
    "\n",
    "rfr_params = {\n",
    "    'n_estimators': np.arange(5,30,5)\n",
    "}\n",
    "\n",
    "br_params = {\n",
    "    'alpha_1': np.logspace(-6, -5, 2),\n",
    "    'alpha_2': np.logspace(-6, -5, 2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_knr = RandomizedSearchCV(knr, knr_params, n_jobs=-1, verbose=1)\n",
    "rs_knr.fit(X_train, y_train)\n",
    "display(rs_knr.score(X_train, y_train))\n",
    "display(rs_knr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfr = RandomForestRegressor(n_jobs=-1)\n",
    "rs_rfr = RandomizedSearchCV(rfr, rfr_params, n_jobs=-1, verbose=1)\n",
    "rs_rfr.fit(X_train, y_train)\n",
    "display(rs_rfr.score(X_train, y_train))\n",
    "display(rs_rfr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rs_br = RandomizedSearchCV(br, br_params, n_jobs=-1, verbose=1)\n",
    "rs_br.fit(X_train, y_train)\n",
    "display(rs_br.score(X_train, y_train))\n",
    "display(rs_br.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs_knr = GridSearchCV(knr, knr_params, n_jobs=-1, verbose=1)\n",
    "gs_knr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to train and score various regressors with varying ngram ranges\n",
    "def try_regressor(data, regr, ngram):\n",
    "    '''\n",
    "    Fits tfidf, svd and regressor to the training data then prints train and test score.\n",
    "    \n",
    "    Parameters:\n",
    "        data - iterable containing X_train, X_test, y_train, y_test\n",
    "        regr - instantiated regressor\n",
    "        ngram - tuple for the ngram_range to be used in TfidfVectorizer\n",
    "    \n",
    "    Returns: regressor fit to the training data\n",
    "    '''\n",
    "    tfidf = TfidfVectorizer(stop_words='english', ngram_range=ngram)\n",
    "    sparse_train = tfidf.fit_transform(data[0])\n",
    "    svd = TruncatedSVD(1000)  # ideally able to use max number of features\n",
    "    X_train = svd.fit_transform(sparse_train)\n",
    "    \n",
    "    print('Training {} with ngram = {}.'.format(regr, ngram))\n",
    "    # fit regressor to training data\n",
    "    regr.fit(X_train, data[2])\n",
    "    \n",
    "    # score the model on the training data\n",
    "    print('Train score: {}'.format(regr.score(X_train, data[2])))\n",
    "    \n",
    "    # transform the test data\n",
    "    sparse_test = tfidf.transform(data[1])\n",
    "    X_test = svd.transform(sparse_test)\n",
    "    \n",
    "    # score the model on the test data\n",
    "    print('Test score: {}'.format(regr.score(X_test, data[3])))\n",
    "    \n",
    "#     return regr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prep data to pass into fitting function\n",
    "data = (X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lasso = Lasso()\n",
    "try_regressor(data, regr=lasso, ngram=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the pipeline\n",
    "linreg_pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words='english')),\n",
    "    ('svd', TruncatedSVD()),\n",
    "    ('regr', LinearRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the parameters dictionary\n",
    "linreg_params = {\n",
    "    'tfidf__norm': ['l1', 'l2'],\n",
    "    'tfidf__ngram_range': [(1,1), (1,2), (1,3)],\n",
    "    'tfidf__min_df': [1, 3, 5],\n",
    "    'svd__n_components': [2, 10, 20]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# perform the grid search\n",
    "linreg_gs = GridSearchCV(linreg_pipe, linreg_params, n_jobs=-1, verbose=1)\n",
    "linreg_gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "def get_gs_results(params, model, xtest, ytest):\n",
    "    print('Parameters used:')\n",
    "    pprint(linreg_params)\n",
    "    best_parameters = model.best_estimator_.get_params()\n",
    "    print('Best parameters:')\n",
    "    for p_name in sorted(params.keys()):\n",
    "        print(\"\\t{}: {}\".format(p_name, best_parameters[p_name]))\n",
    "    print('Train score: {}'.format(model.best_score_))\n",
    "    print('Test score: {}'.format(model.score(xtest,ytest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_gs_results(linreg_params, linreg_gs, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(linreg_gs.cv_results_).sort_values('rank_test_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
