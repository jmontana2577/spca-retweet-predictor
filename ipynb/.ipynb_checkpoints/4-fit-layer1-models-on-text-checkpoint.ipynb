{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook 4 - Fit Regression Models on the Tweet Text\n",
    "The purpose of this notebook is to process the text portion of the tweets (feature extraction and dimensionality reduction) and train regression models for the first layer of the model stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Text Encoder\n",
    "I gathered over one million random tweets from around the US and Canada to use for fitting the text encoder. I did this using the Twitter Stream API and two different AWS instances. To avoid duplicates, one instance pulled tweets from the left half of US and Canada (labeled 'west') and the other pulled from the right half (labeled 'east'). Consult the notebook titled `1-get-tweets-streaming` in the `ipynb` folder for the code. The random tweets are all stored in pickle files (roughly 10,000 tweets per file with some exceptions). Here are the necessary steps to train the encoder:  \n",
    "1. Import text from tweets and create a huge dataframe.\n",
    "1. Clean the text using the cleaner.\n",
    "1. Fit the tf-idf vectorizer using the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # one-time code to import the files that start with east (68 in all)\n",
    "# df = pd.DataFrame()\n",
    "# path = '../data/'\n",
    "# for i in range(69):\n",
    "#     print(i, end='\\r')\n",
    "#     if i < 10:\n",
    "#         filename = path + 'east-0{}.p'.format(i)\n",
    "#     else:\n",
    "#         filname = path + 'east-{}.p'.format(i)\n",
    "    \n",
    "#     df = df.append(pd.read_pickle(filename), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # one-time code to import the files that start with west (35 in all) and append to the previous dataframe\n",
    "# path = '../data/'\n",
    "# for i in range(36):\n",
    "#     print(i, end='\\r')\n",
    "#     if i < 10:\n",
    "#         filename = path + 'west-0{}.p'.format(i)\n",
    "#     else:\n",
    "#         filname = path + 'west-{}.p'.format(i)\n",
    "    \n",
    "#     df = df.append(pd.read_pickle(filename), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # drop rows with empty tweets\n",
    "# df.dropna(subset=['text'], inplace=True)\n",
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # write to pickle so that all the individual files can be deleted\n",
    "# df.to_pickle('../data/big_tweets.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_big = pd.read_pickle('../data/big_tweets.p')\n",
    "df_spca = pd.read_pickle('../data/3-post_eda.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**  \n",
    "Taking a look at some of the text data to see what sort of cleaning needs to be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('\\n\\n'.join(df_spca.text.sample(10).values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of considerations for text cleaning\n",
    "1. **Hashtags and At Symbols:** removing just the symbols themselves, but keeping the phrases attached. Hashtags and mentions are also going to be in a separate data set on which to fit a model and ensemble with other data sets.\n",
    "1. **URL's:** I will get rid of them entirely. Need to note that there are some that begin with \"http\" and some that do not. Email addresses should be get the same treatment.\n",
    "1. **Punctuation:** I will remove all punctuation, which will capture the hashtags and at symbols mentioned above.\n",
    "1. **Capital Letters:** I will convert everything to lower case.\n",
    "1. **Numbers:** I'm going to replace stand-alone numbers to the string \"NUMBER\", but numbers part of a string will remain. For instance, 280 will become NUMBER but kourtneeybell3 will stay the same.\n",
    "1. **Whitespace:** All white space will be replaced with a single space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to clean the input text\n",
    "def cleaner(message):\n",
    "    message = re.sub('https?:\\/{2}\\s?[^\\s]*', '', message) # remove http url's\n",
    "    message = re.sub('[^\\s]+\\/[^\\s]+', '', message) # remove some random strings with /'s\n",
    "    message = re.sub('[^\\s]*\\.com[^\\s]*', '', message) # remove .com that doesn't start with http\n",
    "    message = re.sub('[^\\s]*\\.net[^\\s]*', '', message) # remove .net that doesn't start with http\n",
    "    message = re.sub('\\.+', ' ', message) # replace dots with space\n",
    "    message = re.sub('[^a-z0-9 ]','', message.lower())  # convert to lowercase and remove punctuation\n",
    "    message = re.sub('\\s+\\d+\\s+',' NUMBER ',message) # replace stand-alone numbers with the string \"NUMBER\"\n",
    "    message = re.sub('\\s+',' ',message) # replace whitespace with a single space\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compare pre-processed text to clean text to validate the cleaner function\n",
    "some_tweet_text = df_spca.text.sample(10).values\n",
    "for t in some_tweet_text:\n",
    "    print(t)\n",
    "    print(cleaner(t), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Note:**  \n",
    "Based on several 10-tweet samples, the cleaner appears to be working quite well. One issue I notice is with url's that have multiple spaces. I don't believe there's a way to eliminate those without also eliminating relevant text that follows a url. Here's an example:  \n",
    "\n",
    "```\n",
    "RT @aerocar: @aerocar & @HighendLimo are proud supporters of @pawsforacause ! http:// support.spca.bc.ca/site/TR?pg=ent ry&fr_id=1424 â€¦\n",
    "\n",
    "rt aerocar aerocar highendlimo are proud supporters of pawsforacause ryfrid1424\n",
    "```\n",
    "\n",
    "Overall, I believe the cleaner works sufficiently so time to clean the text for both dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_big['text'] = df_big.text.apply(cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_spca['text'] = df_spca.text.apply(cleaner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Split\n",
    "Because I intend to use stacking, I have to split the approximate 80k rows of data as follows:\n",
    "- 50k to be used to train (40k) and test (10k) the models in the first layer of the stack.  \n",
    "- 30k to be used to train (24k) and test (6k) the blender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40740, 19) (10186, 19) (40740,) (10186,)\n",
      "(23928, 19) (5982, 19) (23928,) (5982,)\n"
     ]
    }
   ],
   "source": [
    "# split the data into layer1 set (63%) and blender set (37%) \n",
    "df_layer1, df_blender, y_layer1, y_blender = train_test_split(df_spca.drop('retweets', axis=1), df_spca.retweets, \n",
    "                                                              test_size=.37, random_state=55)\n",
    "\n",
    "# split each of the layer1 set and blender set into train and test sets\n",
    "df_layer1_train, df_layer1_test, y_layer1_train, y_layer1_test = train_test_split(df_layer1, y_layer1, \n",
    "                                                                                  test_size=.2, random_state=55)\n",
    "df_blender_train, df_blender_test, y_blender_train, y_blender_test = train_test_split(df_blender, y_blender, \n",
    "                                                                                      test_size=.2, random_state=55)\n",
    "\n",
    "print(df_layer1_train.shape, df_layer1_test.shape, y_layer1_train.shape, y_layer1_test.shape)\n",
    "print(df_blender_train.shape, df_blender_test.shape, y_blender_train.shape, y_blender_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/y_blender_test.pkl']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output the split data to pickle files for use in other notebooks\n",
    "joblib.dump(df_layer1_train, '../data/df_layer1_train.pkl')\n",
    "joblib.dump(df_layer1_test, '../data/df_layer1_test.pkl')\n",
    "joblib.dump(y_layer1_train, '../data/y_layer1_train.pkl')\n",
    "joblib.dump(y_layer1_test, '../data/y_layer1_test.pkl')\n",
    "joblib.dump(df_blender_train, '../data/df_blender_train.pkl')\n",
    "joblib.dump(df_blender_test, '../data/df_blender_test.pkl')\n",
    "joblib.dump(y_blender_train, '../data/y_blender_train.pkl')\n",
    "joblib.dump(y_blender_test, '../data/y_blender_test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Various Regressors on the Text Data to Identify Best Options for Layer 1\n",
    "1. Fit the `TfidfVectorizer` to the random tweets with `stop_words='english'` and everything else is default for the first run. Repeat with different values for `ngram_range`, `min_df` and `max_features`.\n",
    "1. Grab just the SPCA text data as the predictor.  \n",
    "1. Transform the SPCA training text using the fit tfidf vectorizer and svd.\n",
    "1. Train various regressors to see which are most appropriate to use for the first layer. I will try the following regressors with default parameters, and the intention is to ultimately tune / boost those that lead to best results:  \n",
    "    1. `Lasso`\n",
    "    1. `RandomForestRegressor`\n",
    "    1. `KNeighborsRegressor`\n",
    "    1. `BayesianRidge`\n",
    "    1. `GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import Lasso, BayesianRidge\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor, RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only need to run this once\n",
    "# fit random twitter text data with tfidf and svd\n",
    "tfidf = TfidfVectorizer(stop_words='english', ngram_range=(1,1), min_df=1, max_features=None)\n",
    "big_sparse = tfidf.fit_transform(df_big.text)\n",
    "display(big_sparse.shape)\n",
    "svd = TruncatedSVD(500)\n",
    "svd.fit(big_sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing TF-IDF, SVD and Data Instead of Splitting and Fitting Again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the prefit tfidf and svd \n",
    "tfidf = joblib.load('../data/4-text_tfidf.pkl')\n",
    "svd = joblib.load('../data/4-text_svd.pkl')\n",
    "\n",
    "# import the layer1 training and test sets\n",
    "df_layer1_train = joblib.load('../data/df_layer1_train.pkl')\n",
    "df_layer1_test = joblib.load('../data/df_layer1_test.pkl')\n",
    "y_layer1_train = joblib.load('../data/y_layer1_train.pkl')\n",
    "y_layer1_test = joblib.load('../data/y_layer1_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the text data from the train and test sets for first layer\n",
    "X_layer1_train = df_layer1_train.text\n",
    "X_layer1_test = df_layer1_test.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to fit and score a regressor\n",
    "def fit_and_score_text(data, regr, return_regr=False):\n",
    "    '''\n",
    "    Transforms the data using the fitted tfidf and svd, fits the regressor to the transformed \n",
    "    training data and then prints train and test scores.\n",
    "    \n",
    "    Parameters:\n",
    "        data - iterable containing X_train, X_test, y_train, y_test\n",
    "        regr - instantiated regressor\n",
    "        return_regr - boolean, option to return the fit regressor (default: False)\n",
    "    \n",
    "    Returns: optional, regressor fit to the transformed training data\n",
    "    '''\n",
    "    \n",
    "    print('Regressor: {}'.format(regr))\n",
    "    train_sparse = tfidf.transform(data[0])\n",
    "    X_train = svd.transform(train_sparse)\n",
    "    \n",
    "    regr.fit(X_train, data[2])\n",
    "    print('Train score: {}'.format(regr.score(X_train, data[2])))\n",
    "    \n",
    "    test_sparse = tfidf.transform(data[1])\n",
    "    X_test = svd.transform(test_sparse)\n",
    "    \n",
    "    print('Test score: {}'.format(regr.score(X_test, data[3])))\n",
    "    \n",
    "    if return_regr:\n",
    "        return regr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = (X_layer1_train, X_layer1_test, y_layer1_train, y_layer1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lasso = Lasso()\n",
    "rfr = RandomForestRegressor(n_jobs=-1)\n",
    "knr = KNeighborsRegressor(n_jobs=-1)\n",
    "bayes = BayesianRidge()\n",
    "gbr = GradientBoostingRegressor()\n",
    "models = [lasso, rfr, knr, bayes, gbr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regressor: Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=None,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Train score: 0.0\n",
      "Test score: -0.00027090916794425546\n",
      "\n",
      "Regressor: RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "           verbose=0, warm_start=False)\n",
      "Train score: 0.816431248796844\n",
      "Test score: 0.38446032879585323\n",
      "\n",
      "Regressor: KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
      "          weights='uniform')\n",
      "Train score: 0.5455633208870001\n",
      "Test score: 0.3532456296611669\n",
      "\n",
      "Regressor: BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True,\n",
      "       fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06, n_iter=300,\n",
      "       normalize=False, tol=0.001, verbose=False)\n",
      "Train score: 0.352643388946975\n",
      "Test score: 0.33563605901279503\n",
      "\n",
      "Regressor: GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
      "             max_leaf_nodes=None, min_impurity_split=1e-07,\n",
      "             min_samples_leaf=1, min_samples_split=2,\n",
      "             min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "             presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "             warm_start=False)\n",
      "Train score: 0.3942151656706052\n",
      "Test score: 0.36494324418160173\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    fit_and_score_text(data, model)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning TF-IDF and SVD \n",
    "I used the base regressor models to compare scores after tuning the tfidf and svd\n",
    "1. `TfidfVectorizer(min_df = 10)` - ever so slightly worse\n",
    "1. `TfidfVectorizer(max_features = 50000)` - ever so slightly worse\n",
    "1. `TfidfVectorizer(ngram_range = (2,2))` - significantly worse\n",
    "1. `TruncatedSVD(n_components=300)` - slightly worse\n",
    "\n",
    "So I will not be making any changes to the tfidf or svd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle the tfidf and svd for later use\n",
    "joblib.dump(tfidf, '../data/4-text_tfidf.pkl')\n",
    "joblib.dump(svd, '../data/4-text_svd.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune / Boost the Top Regressors\n",
    "(All attempts are documented below, but I'm showing only those that were deemed the \"winners\".)\n",
    "\n",
    "#### Record of Tuning / Boosting Attempts and Results\n",
    "1. `RandomForestRegressor`\n",
    "    - `n_estimators`: 5, 15, 25, 50, 75 - optimal at 50 (75 was negligibly better but cost more)  **Winner:** 50 estimators\n",
    "1. `KNeighborsRegressor`\n",
    "    - `n_neighbors`: 3, 7, 9, 11, 13 - results peaked at 11  **Winner:** 11 neighbors\n",
    "1. `BayesianRidge`\n",
    "    - `AdaBoostRegressor` did not improve the scores\n",
    "1. `GradientBoostingRegressor`\n",
    "    - `n_estimators`: 100, 500, 750, 1000 - big jump from 500 to 750, small from 750 to 1000  **Winner:** 1000 estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regressor: RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=50, n_jobs=1, oob_score=False, random_state=None,\n",
      "           verbose=0, warm_start=False)\n",
      "Train score: 0.8393992619203053\n",
      "Test score: 0.43068263192153317\n"
     ]
    }
   ],
   "source": [
    "rfr_50 = RandomForestRegressor(n_estimators=50, n_jobs=-1)\n",
    "fit_and_score_text(data, rfr_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regressor: KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=11, p=2,\n",
      "          weights='uniform')\n",
      "Train score: 0.47126003469565186\n",
      "Test score: 0.3805232310087995\n"
     ]
    }
   ],
   "source": [
    "knr_11 = KNeighborsRegressor(n_neighbors=11, n_jobs=-1)\n",
    "fit_and_score_text(data, knr_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regressor: GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
      "             max_leaf_nodes=None, min_impurity_split=1e-07,\n",
      "             min_samples_leaf=1, min_samples_split=2,\n",
      "             min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "             presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "             warm_start=False)\n",
      "Train score: 0.5915463933550908\n",
      "Test score: 0.4284726172114104\n"
     ]
    }
   ],
   "source": [
    "gbr_1000 = GradientBoostingRegressor(n_estimators=1000)\n",
    "fit_and_score_text(data, gbr_1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the Best Models\n",
    "Exporting the models to be used in the blender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/4-text_gbr.pkl']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(rfr_50, '../data/4-text_rfr.pkl')\n",
    "joblib.dump(knr_11, '../data/4-text_knr.pkl')\n",
    "joblib.dump(bayes, '../data/4-text_bayes.pkl')\n",
    "joblib.dump(gbr_1000, '../data/4-text_gbr.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
