{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook 6 - Fit Regression Models on the Hashtags and Mentions in the Tweets \n",
    "The purpose of this notebook is to train regression models on the hashtags and mentions in the tweets for the first layer of the model stack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Future Consideration:\n",
    "Fit the vectorizer on the random tweets and the SPCA tweets together instead of just the SPCA tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps to Work Through\n",
    "1. Using SPCA dataset, combine each tweet's hashtags and mentions into one sentence **(write a function for this since it will be used on the training and test sets as well)**\n",
    "1. Fit and transform the sentences using count vectorizer then fit the svd\n",
    "1. Combine hashtags and mentions for the training and test sets\n",
    "1. Transform the sentences in the training and test sets using the fit count vectorizer and svd.\n",
    "1. Try different regression models to identify best options\n",
    "1. Boost the best options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Combine Hashtags and Mentions for SPCA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get all the SPCA data for the purpose of fitting count vectorizer and svd\n",
    "df_spca = pd.read_pickle('../data/3-post_eda.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write function to combine hashtags and mentions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run the above function on the SPCA data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Count Vectorize and do SVD on the Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Need code here to train count vectorizer and svd on all SPCA data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Combine Hashtags and Mentions for Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the layer1 training and test sets\n",
    "df_layer1_train = joblib.load('../data/df_layer1_train.pkl')\n",
    "df_layer1_test = joblib.load('../data/df_layer1_test.pkl')\n",
    "y_layer1_train = joblib.load('../data/y_layer1_train.pkl')\n",
    "y_layer1_test = joblib.load('../data/y_layer1_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run combining function on training and test sets\n",
    "# use X_hash_train, X_hash_test as names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Transform the Sentences for Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform the training and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Train Various Regressors on the Hashtags and Mentions to Identify Best Options for Layer 1\n",
    "Train various regressors to see which are most appropriate to use for the first layer. I will try the following regressors with default parameters, and the intention is to ultimately boost those that lead to best results:  \n",
    "   1. `Lasso`\n",
    "   1. `DecisionTreeRegressor`\n",
    "   1. `KNeighborsRegressor`\n",
    "   1. `BayesianRidge`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import Lasso, BayesianRidge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to fit and score a regressor\n",
    "def fit_and_score(data, regr, return_regr=False):\n",
    "    '''\n",
    "    Transforms the data using the pre-fit tfidf and svd, fits the regressor to the transformed \n",
    "    training data and then prints train and test scores.\n",
    "    \n",
    "    Parameters:\n",
    "        data - iterable containing X_train, X_test, y_train, y_test\n",
    "        regr - instantiated regressor\n",
    "        return_regr - boolean, option to return the fit regressor (default: False)\n",
    "    \n",
    "    Returns: optional, regressor fit to the transformed training data\n",
    "    '''\n",
    "    \n",
    "    print('Regressor: {}'.format(regr))\n",
    "    train_sparse = tfidf.transform(data[0])\n",
    "    X_train = svd.transform(train_sparse)\n",
    "    \n",
    "    regr.fit(X_train, data[2])\n",
    "    print('Train score: {}'.format(regr.score(X_train, data[2])))\n",
    "    \n",
    "    test_sparse = tfidf.transform(data[1])\n",
    "    X_test = svd.transform(test_sparse)\n",
    "    \n",
    "    print('Test score: {}'.format(regr.score(X_test, data[3])))\n",
    "    \n",
    "    if return_regr:\n",
    "        return regr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = (X_hash_train, X_hash_test, ylayer1_train, y_layer1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lasso = Lasso()\n",
    "dtr = DecisionTreeRegressor()\n",
    "knr = KNeighborsRegressor(n_jobs=-1)\n",
    "bayes = BayesianRidge()\n",
    "models = [lasso, dtr, knr, bayes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    fit_and_score(data, model)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 6. Boost the Top Regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
