{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook 6 - Fit Regression Models on the Hashtags and Mentions in the Tweets \n",
    "The purpose of this notebook is to train regression models on the hashtags and mentions in the tweets for the first layer of the model stack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Future Consideration:\n",
    "Fit the vectorizer on the random tweets and the SPCA tweets together instead of just the SPCA tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps to Work Through\n",
    "1. Fit and transform the entire SPCA data hashtags and mentions sentences using count vectorizer then fit the svd\n",
    "1. Try different regression models on the transformed sentences to identify best options\n",
    "1. Tune / Boost the best options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Count Vectorize and do SVD on the Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get all the SPCA data for the purpose of fitting count vectorizer and svd\n",
    "df_spca = pd.read_pickle('../data/3-post_eda.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80836, 27454)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=500, n_iter=5,\n",
       "       random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit count vectorizer and svd on all SPCA data\n",
    "X = df_spca.hashtags_and_mentions\n",
    "tags_cv = CountVectorizer(stop_words='english')\n",
    "tags_sparse = tags_cv.fit_transform(X)\n",
    "display(tags_sparse.shape)\n",
    "tags_svd = TruncatedSVD(500)\n",
    "tags_svd.fit(tags_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/6-tag_svd.pkl']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pickle the cv and svd for later use\n",
    "joblib.dump(tags_cv, '../data/6-tags_cv.pkl')\n",
    "joblib.dump(tags_svd, '../data/6-tags_svd.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train Various Regressors on the Hashtags and Mentions to Identify Best Options for Layer 1\n",
    "Train various regressors to see which are most appropriate to use for the first layer. I will try the following regressors with default parameters, and the intention is to ultimately tune / boost those that lead to best results:  \n",
    "   1. `Lasso`\n",
    "   1. `DecisionTreeRegressor`\n",
    "   1. `KNeighborsRegressor`\n",
    "   1. `BayesianRidge`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the layer1 training and test sets\n",
    "df_layer1_train = joblib.load('../data/df_layer1_train.pkl')\n",
    "df_layer1_test = joblib.load('../data/df_layer1_test.pkl')\n",
    "y_layer1_train = joblib.load('../data/y_layer1_train.pkl')\n",
    "y_layer1_test = joblib.load('../data/y_layer1_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get hashtags and mentions from the training and test sets\n",
    "X_tags_train = df_layer1_train.hashtags_and_mentions\n",
    "X_tags_test = df_layer1_test.hashtags_and_mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, BayesianRidge\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to fit and score a regressor\n",
    "def fit_and_score_tags(data, regr, return_regr=False):\n",
    "    '''\n",
    "    Transforms the hashtags and mentions sentences using the fitted cv and svd, fits the regressor \n",
    "    to the transformed training data and then prints train and test scores.\n",
    "    \n",
    "    Parameters:\n",
    "        data - iterable containing X_train, X_test, y_train, y_test\n",
    "        regr - instantiated regressor\n",
    "        return_regr - boolean, option to return the fit regressor (default: False)\n",
    "    \n",
    "    Returns: optional, regressor fit to the transformed training data\n",
    "    '''\n",
    "    \n",
    "    print('Regressor: {}'.format(regr))\n",
    "    train_sparse = tag_cv.transform(data[0])\n",
    "    X_train = tag_svd.transform(train_sparse)\n",
    "    \n",
    "    regr.fit(X_train, data[2])\n",
    "    print('Train score: {}'.format(regr.score(X_train, data[2])))\n",
    "    \n",
    "    test_sparse = tag_cv.transform(data[1])\n",
    "    X_test = tag_svd.transform(test_sparse)\n",
    "    \n",
    "    print('Test score: {}'.format(regr.score(X_test, data[3])))\n",
    "    \n",
    "    if return_regr:\n",
    "        return regr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (X_tags_train, X_tags_test, y_layer1_train, y_layer1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lasso = Lasso()\n",
    "rfr = RandomForestRegressor(n_jobs=-1)\n",
    "knr = KNeighborsRegressor(n_jobs=-1)\n",
    "bayes = BayesianRidge()\n",
    "gbr = GradientBoostingRegressor()\n",
    "models = [lasso, rfr, knr, bayes, gbr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regressor: Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=None,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Train score: 0.0\n",
      "Test score: -0.00027090916794425546\n",
      "\n",
      "Regressor: RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=10, n_jobs=-1, oob_score=False, random_state=None,\n",
      "           verbose=0, warm_start=False)\n",
      "Train score: 0.5106353315805702\n",
      "Test score: 0.297496539336915\n",
      "\n",
      "Regressor: KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
      "          weights='uniform')\n",
      "Train score: 0.37709008773048036\n",
      "Test score: 0.21891598446317803\n",
      "\n",
      "Regressor: BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True,\n",
      "       fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06, n_iter=300,\n",
      "       normalize=False, tol=0.001, verbose=False)\n",
      "Train score: 0.26029766137509514\n",
      "Test score: 0.24468923248438046\n",
      "\n",
      "Regressor: GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
      "             max_leaf_nodes=None, min_impurity_split=1e-07,\n",
      "             min_samples_leaf=1, min_samples_split=2,\n",
      "             min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "             presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "             warm_start=False)\n",
      "Train score: 0.3032761422791336\n",
      "Test score: 0.2819022264168475\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    fit_and_score_tags(data, model)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regressor: RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "           min_samples_split=50, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=50, n_jobs=-1, oob_score=False, random_state=None,\n",
      "           verbose=0, warm_start=False)\n",
      "Train score: 0.452209613957352\n",
      "Test score: 0.33014432220611045\n"
     ]
    }
   ],
   "source": [
    "rfr_50_50 = RandomForestRegressor(n_estimators=50, min_samples_split=50, n_jobs=-1)\n",
    "fit_and_score_tags(data, rfr_50_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regressor: KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=9, p=2,\n",
      "          weights='uniform')\n",
      "Train score: 0.33601431474104726\n",
      "Test score: 0.2387399564012711\n"
     ]
    }
   ],
   "source": [
    "knr_9 = KNeighborsRegressor(n_neighbors=9, n_jobs=-1)\n",
    "fit_and_score_tags(data, knr_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regressor: GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
      "             max_leaf_nodes=None, min_impurity_split=1e-07,\n",
      "             min_samples_leaf=1, min_samples_split=2,\n",
      "             min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "             presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "             warm_start=False)\n",
      "Train score: 0.37664554243644355\n",
      "Test score: 0.31815056740618075\n"
     ]
    }
   ],
   "source": [
    "gbr_500 = GradientBoostingRegressor(n_estimators=500)\n",
    "fit_and_score_tags(data, gbr_500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4. Tune / Boost the Top Regressors\n",
    "(All attempts are documented below, but I'm showing only those that were deemed the \"winners\".)\n",
    "\n",
    "#### Record of Tuning / Boosting Attempts and Results\n",
    "1. `RandomForestRegressor`\n",
    "    1. `n_estimators`: 5, 15, 25, 50, 75, 100 - continued improvement but negligible from 50 to 100 \n",
    "    1. `min_samples_split`: 25, 50, 75 - results peaked at 50  \n",
    "    **Winner:** 50 estimators, 50 samples to split  \n",
    "1. `KNeighborsRegressor`\n",
    "    1. `n_neighbors`: 3, 7, 9 - 9 got biggest bump in test score  \n",
    "1. `BayesianRidge`\n",
    "    1. `AdaBoostRegressor` did not improve the scores\n",
    "1. `GradientBoostingRegressor`\n",
    "    1. `n_estimators`: 100, 500, 1000 - results peaked at 500\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the Best Models\n",
    "Exporting the models to be used in the blender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/6-tags_gbr.pkl']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(rfr_50_50, '../data/6-tags_rfr.pkl')\n",
    "joblib.dump(knr_9, '../data/6-tags_knr.pkl')\n",
    "joblib.dump(bayes, '../data/6-tags_bayes.pkl')\n",
    "joblib.dump(gbr_500, '../data/6-tags_gbr.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
