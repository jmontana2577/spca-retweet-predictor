{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook 4 - Fit Models on the Tweet Text\n",
    "The purpose of this notebook is to process the text portion of the tweets (feature extraction and dimensionality reduction) and train models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Text Encoder\n",
    "I gathered over one million random tweets from around the US and Canada to use for fitting the text encoder. I did this using the Twitter Stream API and two different AWS instances. To avoid duplicates, one pulled tweets from the left half of US and Canada (labeled 'west') and the other pulled from the right half (labeled 'east'). Consult the notebook titled `1-get-tweets-streaming` in the `ipynb` folder for the code. The random tweets are all stored in pickle files (roughly 10,000 tweets per file with some exceptions). Here are the necessary steps to train the encoder:  \n",
    "1. Import text from tweets and create a huge dataframe.\n",
    "1. Clean the text using the cleaner.\n",
    "1. Fit the tf-idf vectorizer using the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # one-time code to import the files that start with east (68 in all)\n",
    "# df = pd.DataFrame()\n",
    "# path = '../data/'\n",
    "# for i in range(69):\n",
    "#     print(i, end='\\r')\n",
    "#     if i < 10:\n",
    "#         filename = path + 'east-0{}.p'.format(i)\n",
    "#     else:\n",
    "#         filname = path + 'east-{}.p'.format(i)\n",
    "    \n",
    "#     df = df.append(pd.read_pickle(filename)[['text']], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # one-time code to import the files that start with west (35 in all) and append to the dataframe\n",
    "# path = '../data/'\n",
    "# for i in range(36):\n",
    "#     print(i, end='\\r')\n",
    "#     if i < 10:\n",
    "#         filename = path + 'west-0{}.p'.format(i)\n",
    "#     else:\n",
    "#         filname = path + 'west-{}.p'.format(i)\n",
    "    \n",
    "#     df = df.append(pd.read_pickle(filename)[['text']], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # write to pickle so that all the individual files can be deleted\n",
    "# df.to_pickle('../data/big_tweets.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_big = pd.read_pickle('../data/big_tweets.p')\n",
    "df = pd.read_pickle('../data/3-post_eda.p')\n",
    "df_big.shape, df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**  \n",
    "Taking a look at some of the text data to see what sort of cleaning needs to be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print('\\n\\n'.join(df.text.sample(10).values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of considerations for text cleaning\n",
    "1. **Hashtags and At Symbols:** removing just the symbols themselves, but keeping the phrases attached. Hashtags and mentions are also going to be in a separate data set on which to fit a model and ensemble with other data sets.\n",
    "1. **URL's:** I will get rid of them entirely. Need to note that there are some that begin with \"http\" and some that do not. Email addresses should be get the same treatment.\n",
    "1. **Punctuation:** I will remove all punctuation, which will capture the hashtags and at symbols mentioned above.\n",
    "1. **Capital Letters:** I will convert everything to lower case.\n",
    "1. **Numbers:** I'm going to replace stand-alone numbers to the string \"NUMBER\", but numbers part of a string will remain. For instance, 280 will become NUMBER but kourtneeybell3 will stay the same.\n",
    "1. **Whitespace:** All white space will be replaced with a single space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to clean the input text\n",
    "def cleaner(message):\n",
    "    message = re.sub('https?:\\/{2}\\s?[^\\s]*', '', message) # remove http url's\n",
    "    message = re.sub('[^\\s]+\\/[^\\s]+', '', message) # remove some random strings with /'s\n",
    "    message = re.sub('[^\\s]*\\.com[^\\s]*', '', message) # remove .com that doesn't start with http\n",
    "    message = re.sub('[^\\s]*\\.net[^\\s]*', '', message) # remove .net that doesn't start with http\n",
    "    message = re.sub('\\.+', ' ', message) # replace dots with space\n",
    "    message = re.sub('[^a-z0-9 ]','', message.lower())  # convert to lowercase and remove punctuation\n",
    "    message = re.sub('\\s+\\d+\\s+',' NUMBER ',message) # replace stand-alone numbers with the string \"NUMBER\"\n",
    "    message = re.sub('\\s+',' ',message) # replace whitespace with a single space\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare pre-processed text to clean text to validate the cleaner function\n",
    "some_tweet_text = df.text.sample(10).values\n",
    "for t in some_tweet_text:\n",
    "    print(t)\n",
    "    print(cleaner(t), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Note:**  \n",
    "Based on several 10-tweet samples, the cleaner appears to be working quite well. One issue I notice is with url's that have multiple spaces. I don't believe there's a way to eliminate those without also eliminating relevant text that follows a url. Here's an example:  \n",
    "\n",
    "```\n",
    "RT @aerocar: @aerocar & @HighendLimo are proud supporters of @pawsforacause ! http:// support.spca.bc.ca/site/TR?pg=ent ry&fr_id=1424 â€¦\n",
    "\n",
    "rt aerocar aerocar highendlimo are proud supporters of pawsforacause ryfrid1424\n",
    "```\n",
    "\n",
    "Overall, I believe the cleaner works sufficiently so time to clean the text for both dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_big['text'] = df_big.text.apply(cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['text'] = df.text.apply(cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split the data into train and test sets\n",
    "df_train, df_test, y_train, y_test = train_test_split(df.drop('retweets', axis=1), df.retweets, \n",
    "                                                      test_size=.3, random_state=55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Various Regressors on the Text Data to Identify Best Options \n",
    "1. Fit the `TfidfVectorizer` to the random tweets with `stop_words='english'` and everything else is default.\n",
    "1. Grab just the SPCA text data as the predictor.  \n",
    "1. Transform the SPCA training text using the fit tfidf vectorizer. Check the dimensions of the sparse matrix to see if adjustments need to be made to the vectorizer (i.e. ngram_range, min_df, max_features)\n",
    "1. Perform dimensionality reduction using `TruncatedSVD` with all components.\n",
    "1. Train various regressors to see which are most appropriate to try to tune. I will try the following regressors:  \n",
    "    1. `Lasso`\n",
    "    1. `DecisionTreeRegressor`\n",
    "    1. `KNeighborsRegressor`\n",
    "    1. `BayesianRidge`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LinearRegression, Lasso, BayesianRidge\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(stop_words='english', ngram_range=(1,1), min_df=1, max_features=None)\n",
    "tfidf.fit(df_big.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the text data from the train and test sets\n",
    "X_train = df_train.text\n",
    "X_test = df_test.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_train = tfidf.transform(X_train)\n",
    "sparse_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to train and score various regressors with varying ngram ranges\n",
    "def try_regressor(data, regr, ngram):\n",
    "    '''\n",
    "    Fits tfidf, svd and regressor to the training data then prints train and test score.\n",
    "    \n",
    "    Parameters:\n",
    "        data - iterable containing X_train, X_test, y_train, y_test\n",
    "        regr - instantiated regressor\n",
    "        ngram - tuple for the ngram_range to be used in TfidfVectorizer\n",
    "    \n",
    "    Returns: regressor fit to the training data\n",
    "    '''\n",
    "    tfidf = TfidfVectorizer(stop_words='english', ngram_range=ngram)\n",
    "    sparse_train = tfidf.fit_transform(data[0])\n",
    "    svd = TruncatedSVD(1000)  # ideally able to use max number of features\n",
    "    X_train = svd.fit_transform(sparse_train)\n",
    "    \n",
    "    print('Training {} with ngram = {}.'.format(regr, ngram))\n",
    "    # fit regressor to training data\n",
    "    regr.fit(X_train, data[2])\n",
    "    \n",
    "    # score the model on the training data\n",
    "    print('Train score: {}'.format(regr.score(X_train, data[2])))\n",
    "    \n",
    "    # transform the test data\n",
    "    sparse_test = tfidf.transform(data[1])\n",
    "    X_test = svd.transform(sparse_test)\n",
    "    \n",
    "    # score the model on the test data\n",
    "    print('Test score: {}'.format(regr.score(X_test, data[3])))\n",
    "    \n",
    "#     return regr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prep data to pass into fitting function\n",
    "data = (X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lasso = Lasso()\n",
    "try_regressor(data, regr=lasso, ngram=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the pipeline\n",
    "linreg_pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words='english')),\n",
    "    ('svd', TruncatedSVD()),\n",
    "    ('regr', LinearRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the parameters dictionary\n",
    "linreg_params = {\n",
    "    'tfidf__norm': ['l1', 'l2'],\n",
    "    'tfidf__ngram_range': [(1,1), (1,2), (1,3)],\n",
    "    'tfidf__min_df': [1, 3, 5],\n",
    "    'svd__n_components': [2, 10, 20]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# perform the grid search\n",
    "linreg_gs = GridSearchCV(linreg_pipe, linreg_params, n_jobs=-1, verbose=1)\n",
    "linreg_gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "def get_gs_results(params, model, xtest, ytest):\n",
    "    print('Parameters used:')\n",
    "    pprint(linreg_params)\n",
    "    best_parameters = model.best_estimator_.get_params()\n",
    "    print('Best parameters:')\n",
    "    for p_name in sorted(params.keys()):\n",
    "        print(\"\\t{}: {}\".format(p_name, best_parameters[p_name]))\n",
    "    print('Train score: {}'.format(model.best_score_))\n",
    "    print('Test score: {}'.format(model.score(xtest,ytest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_gs_results(linreg_params, linreg_gs, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(linreg_gs.cv_results_).sort_values('rank_test_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
