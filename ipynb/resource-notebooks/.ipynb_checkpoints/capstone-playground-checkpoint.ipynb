{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /home/jovyan/capstone/GetOldTweets-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import got3 as got\n",
    "from pprint import pprint "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at JSON file to see if I can filter out tweets that are replies to other tweets. Also looking for geo and username fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Update:**  Added functionality to skip replies. Found username field but couldn't find any location information (geo, coordinates, location)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request, urllib.parse, urllib.error, json, re, datetime, sys, http.cookiejar\n",
    "from got3 import models\n",
    "from pyquery import PyQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "username = 'pspca'\n",
    "since = '2017-09-03'\n",
    "until = '2017-09-07'\n",
    "\n",
    "# max_tweets = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is the beginning of getTweets, including some of the default parameters\n",
    "tweetCriteria = got.manager.TweetCriteria().setUsername(username).setSince(since).setUntil(until)\n",
    "\n",
    "receiveBuffer = None  # parameter passed to getTweets\n",
    "bufferLength=100 # parameter passed to getTweets\n",
    "proxy=None # parameter passed to getTweets\n",
    "refreshCursor = ''\n",
    "results = []\n",
    "resultsAux = []\n",
    "cookieJar = http.cookiejar.CookieJar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is all the functionality in getJsonResponse which is called inside getTweets\n",
    "url = \"https://twitter.com/i/search/timeline?f=tweets&q=%s&src=typd&%smax_position=%s\"\n",
    "\n",
    "urlGetData = ''\n",
    "if hasattr(tweetCriteria, 'username'):\n",
    "\turlGetData += ' from:' + tweetCriteria.username\n",
    "\t\n",
    "if hasattr(tweetCriteria, 'since'):\n",
    "\turlGetData += ' since:' + tweetCriteria.since\n",
    "\t\n",
    "if hasattr(tweetCriteria, 'until'):\n",
    "\turlGetData += ' until:' + tweetCriteria.until\n",
    "\t\n",
    "if hasattr(tweetCriteria, 'querySearch'):\n",
    "\turlGetData += ' ' + tweetCriteria.querySearch\n",
    "\t\n",
    "if hasattr(tweetCriteria, 'lang'):\n",
    "\turlLang = 'lang=' + tweetCriteria.lang + '&'\n",
    "else:\n",
    "\turlLang = ''\n",
    "url = url % (urllib.parse.quote(urlGetData), urlLang, refreshCursor)\n",
    "print(url)\n",
    "headers = [\n",
    "\t('Host', \"twitter.com\"),\n",
    "\t('User-Agent', \"Mozilla/5.0 (Windows NT 6.1; Win64; x64)\"),\n",
    "\t('Accept', \"application/json, text/javascript, */*; q=0.01\"),\n",
    "\t('Accept-Language', \"de,en-US;q=0.7,en;q=0.3\"),\n",
    "\t('X-Requested-With', \"XMLHttpRequest\"),\n",
    "\t('Referer', url),\n",
    "\t('Connection', \"keep-alive\")\n",
    "]\n",
    "\n",
    "if proxy:\n",
    "\topener = urllib.request.build_opener(urllib.request.ProxyHandler({'http': proxy, 'https': proxy}), urllib.request.HTTPCookieProcessor(cookieJar))\n",
    "else:\n",
    "\topener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cookieJar))\n",
    "opener.addheaders = headers\n",
    "\n",
    "response = opener.open(url)\n",
    "jsonResponse = response.read()\n",
    "\n",
    "dataJson = json.loads(jsonResponse.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataJson['items_html'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pprint(dataJson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# how to see if tweet is a reply\n",
    "tweetPQ(\"div.tweet\").attr(\"data-is-reply-to\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# how to get username from tweet\n",
    "tweetPQ(\"div.tweet\").attr(\"data-screen-name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that I got json file, look at how getTweets gets the attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "refreshCursor = dataJson['min_position']\n",
    "tweets = PyQuery(dataJson['items_html'])('div.js-stream-tweet')\n",
    "\n",
    "for tweetHTML in tweets:\n",
    "    tweetPQ = PyQuery(tweetHTML)\n",
    "    tweet = models.Tweet()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data frame to see what gets outputted in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet_list = got.manager.TweetManager.getTweets(tweetCriteria)\n",
    "\n",
    "from collections import defaultdict\n",
    "d = defaultdict(list)\n",
    "attributes = ['date', 'favorites', 'retweets', 'hashtags', 'geo', 'id',\n",
    "                  'mentions', 'text', 'urls', 'username', 'permalink', 'author_id']\n",
    "for tweet in tweet_list:\n",
    "    for att in attributes:\n",
    "        d[att].append(eval(\"tweet.\" + att))\n",
    "\n",
    "df = pd.DataFrame(d)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this adjusts the hour for time zone difference since API gives it in UTC\n",
    "def adjust_hour(hour, offset):\n",
    "    if hour >= offset:\n",
    "        hour -= offset\n",
    "    else:\n",
    "        hour += 24 - offset\n",
    "    return hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# splits the datetime column into separate features\n",
    "def add_date_features(df, offset=0):\n",
    "    df['year'] = df.date.apply(lambda x: x.year)\n",
    "    df['month'] = df.date.apply(lambda x: x.month)\n",
    "    df['weekday'] = df.date.apply(lambda x: x.weekday())\n",
    "    hours = df.date.apply(lambda x: x.hour)\n",
    "    df['local_hour'] = hours.apply(adjust_hour, args=(offset,))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_date_features(df, offset=4)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../data/all_tweets.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['local_hour'] < 5) & (df['year'] == 2017)][['date', 'username', 'local_hour', 'year']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['year']==2008]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing around with time zone converstions making datetime aware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the time zones\n",
    "utc_zone = pytz.timezone('UTC')\n",
    "et_zone = pytz.timezone('US/Eastern')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a timestamp from the dataframe\n",
    "test = df.date[0]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts pandas timestamp to datetime aware object\n",
    "test_utc = test.to_pydatetime().replace(tzinfo=utc_zone)\n",
    "test_utc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the utc to eastern\n",
    "eastern = test_utc.astimezone(et_zone)\n",
    "eastern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find a date that is outside dst\n",
    "test2 = df.loc[(df['month'] == 1) & (df['username'] == 'PSPCA')].iloc[0,1]\n",
    "test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2_utc = test2.to_pydatetime().replace(tzinfo=utc_zone)\n",
    "test2_utc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IT WORKS!!!!!\n",
    "eastern2 = test2_utc.astimezone(et_zone)\n",
    "eastern2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test the function to be used in the project\n",
    "\n",
    "# get the time zones\n",
    "et_zone = pytz.timezone('US/Eastern')\n",
    "ct_zone = pytz.timezone('US/Central')\n",
    "mt_zone = pytz.timezone('US/Mountain')\n",
    "pt_zone = pytz.timezone('US/Pacific')\n",
    "\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "# function that replaces pandas timestamp with timezone adjusted datetime aware object\n",
    "def convert_to_datetime_aware(date, tz=None):\n",
    "    '''\n",
    "    Parameters: \n",
    "        date - pandas UTC timestamp , \n",
    "        tz - a pytz time zone to convert to (default is None)\n",
    "    \n",
    "    Returns:\n",
    "        datetime aware object in specified time zone (UTC if None)\n",
    "    \n",
    "    '''\n",
    "    utc_zone = pytz.timezone('UTC')\n",
    "    date = date.to_pydatetime().replace(tzinfo=utc_zone)\n",
    "    \n",
    "    if tz:\n",
    "        return date.astimezone(tz)\n",
    "    \n",
    "    return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = convert_to_datetime_aware(test, et_zone)\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sfspca = pd.read_pickle('../data/sfspca_tweets.p')\n",
    "df_sfspca.date.apply(convert_to_datetime_aware, args=(pt_zone,)).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sfspca.date.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sfspca.date.apply(convert_to_datetime_aware).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result:**  \n",
    "The `pytz` looks to be working well, accounting for timezones and dst."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New functionality to account for end date not being inclusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ADJUSTMENT FOR UNTIL NOT BEING INCLUSIVE, SO START EACH MONTH WITH END OF PREVIOUS MONTH\n",
    "#     years = ['2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017']\n",
    "#     months = [('01', '31'), ('02', '28'), ('03', '31'), ('04', '30'), ('05', '31'), ('06', '30'),\n",
    "#               ('07', '31'), ('08', '31'), ('09', '30'), ('10', '31'), ('11', '30'), ('12', '31')]\n",
    "#     total = 0\n",
    "#     for year in years:\n",
    "#         for month in months:\n",
    "#             if year == '2008' and month[0] == '01':\n",
    "#                 since = year + '-' + month[0] + '-01'\n",
    "#             else:\n",
    "#                 since = until\n",
    "#             until = year + '-' + month[0] + '-' + month[1]\n",
    "#             print(since, until)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 9/6/17 - Playing around with Twittercounter API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from urllib.parse import urlencode\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_sfspca = pd.read_pickle('../data/sfspca_tweets.p')\n",
    "df_pspca = pd.read_pickle('../data/pspca_tweets.p')\n",
    "df_houston = pd.read_pickle('../data/houston_tweets.p')\n",
    "df_texas = pd.read_pickle('../data/texas_tweets.p')\n",
    "df_tulsa = pd.read_pickle('../data/tulsa_tweets.p')\n",
    "df_richmond = pd.read_pickle('../data/richmond_tweets.p')\n",
    "df_ontario = pd.read_pickle('../data/ontario_tweets.p')\n",
    "df_alberta = pd.read_pickle('../data/alberta_tweets.p')\n",
    "df_bc = pd.read_pickle('../data/bc_tweets.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'http://api.twittercounter.com'\n",
    "api_key = '7e7c4964440d3b1f61cdc34a0fbb3576'\n",
    "user_id = '23483274'\n",
    "r = requests.get(url + '?' + urlencode({'apikey':api_key, 'twitter_id':user_id}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = r.json()\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['followersperdate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['follow_days']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result:**  \n",
    "Can't go back far enough so stretch goal would be to subset the data so that it only goes back far enough to track number of followers. Then I can include that in my metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Looking at text to create cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../data/post_eda.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = df.text.values[:5]\n",
    "for t in all_text:\n",
    "    print(t)\n",
    "    print(len(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to clean the input text\n",
    "def cleaner(message):\n",
    "    message = re.sub('http[^\\s]+', '', message) # remove url's\n",
    "    message = re.sub('[^\\s]*\\.com[^\\s]*', '', message) # remove .com that doesn't start with http\n",
    "    message = re.sub('[^\\s]*\\.net[^\\s]*', '', message) # remove .net that doesn't start with http\n",
    "    message = re.sub('\\.+', ' ', message) # replace dots with space\n",
    "    message = re.sub('[^a-z0-9 ]','', message.lower())  # convert to lowercase and remove punctuation\n",
    "    message = re.sub('\\d+','NUMBER ',message) # replace numbers with the string \"NUMBER\"\n",
    "    message = re.sub('\\s+',' ',message) # replace whitespace with a single space\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_text = df.text.values[10000:10050]\n",
    "for t in all_text:\n",
    "    print(t)\n",
    "    print(cleaner(t))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner(all_text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Fitting models to the big Twitter data to compare performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_big = pd.read_pickle('../data/big_tweets.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to clean the input text\n",
    "def cleaner(message):\n",
    "    message = re.sub('https?:\\/{2}\\s?[^\\s]*', '', message) # remove http url's\n",
    "    message = re.sub('[^\\s]+\\/[^\\s]+', '', message) # remove some random strings with /'s\n",
    "    message = re.sub('[^\\s]*\\.com[^\\s]*', '', message) # remove .com that doesn't start with http\n",
    "    message = re.sub('[^\\s]*\\.net[^\\s]*', '', message) # remove .net that doesn't start with http\n",
    "    message = re.sub('\\.+', ' ', message) # replace dots with space\n",
    "    message = re.sub('[^a-z0-9 ]','', message.lower())  # convert to lowercase and remove punctuation\n",
    "    message = re.sub('\\s+\\d+\\s+',' NUMBER ',message) # replace stand-alone numbers with the string \"NUMBER\"\n",
    "    message = re.sub('\\s+',' ',message) # replace whitespace with a single space\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_big['text'] = df_big.text.apply(cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_big.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Oops!** I kept only the text data of the million plus tweets to save space. Thinking I should have kept number of retweets too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_big.text, df_big.retweets, \n",
    "                                                      test_size=.3, random_state=55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LinearRegression, Lasso, BayesianRidge\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit random text data with tfidf and svd\n",
    "tfidf = TfidfVectorizer(stop_words='english', ngram_range=(1,1), min_df=1, max_features=None)\n",
    "X_train = tfidf.fit_transform(df_big.text)\n",
    "display(big_sparse.shape)\n",
    "svd = TruncatedSVD(500)\n",
    "svd.fit(big_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_and_score(data, regr):\n",
    "    print('Regressor: {}'.format(regr))\n",
    "    train_sparse = tfidf.transform(data[0])\n",
    "    X_train = svd.transform(train_sparse)\n",
    "    \n",
    "    regr.fit(X_train, data[2])\n",
    "    print('Train score: {}'.format(regr.score(X_train, data[2])))\n",
    "    \n",
    "    test_sparse = tfidf.transform(data[1])\n",
    "    X_test = svd.transform(test_sparse)\n",
    "    \n",
    "    print('Test score: {}'.format(regr.score(X_test, data[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = (X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lasso = Lasso()\n",
    "dtr = DecisionTreeRegressor()\n",
    "knr = KNeighborsRegressor(n_jobs=-1)\n",
    "bayes = BayesianRidge()\n",
    "models = [lasso, dtr, knr, bayes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    fit_and_score(data, model)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
